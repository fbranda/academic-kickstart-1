{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size,out_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.linear_layer = nn.Linear(latent_size, out_size)\n",
    "        nn.init.xavier_normal_(self.layer.weight)\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        input = torch.cat([z,y],-1)\n",
    "        \n",
    "        return self.linear_layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-6-cc9dce9e7c48>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-cc9dce9e7c48>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class Encoder_z(nn.Module):\n",
    "    def __init__(self, input_size,num_classes,latent_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.linear_layer = nn.Linear(input_size + num_classes, 2*latent_size)\n",
    "        nn.init.xavier_normal_(self.linear_layer.weight)\n",
    "        \n",
    "    def _sample_latent(self, mu, log_sigma):\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        epsilon = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())\n",
    "                                   ,requires_grad=False).to(sigma.device)\n",
    "\n",
    "        return mu + epsilon*sigma\n",
    "        \n",
    "    def forward(self, x):\n",
    "        temp_out = self.linear_layer(x)\n",
    "    \n",
    "        mu = temp_out[:, :self.latent_size]\n",
    "        log_sigma = temp_out[:, self.latent_size:]\n",
    "\n",
    "        z = _sample_latent(mu, log_sigma)\n",
    "\n",
    "        return z, mu, log_sigma\n",
    "    \n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size,num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.linear_layer = nn.Linear(input_size, 2*latent_size)\n",
    "        nn.init.xavier_normal_(self.layer.weight)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self_softmax(self-linear(layer(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_loss(x,encoder,decoder,classifier, y_prior=1):\n",
    "    y_q = classifier(x)\n",
    "    kld_cat = torch.mean(torch.sum(y_q*(torch.log(y_q) - torch.log(y_prior)),-1),-1)  \n",
    "    kld_norm = 0\n",
    "    \n",
    "    num_classes = y_q.size(-1)\n",
    "    e = torch.zeros(y_q.size()).to(x.device)\n",
    "    \n",
    "    log_prob_e = []\n",
    "    for j in range(num_classes):\n",
    "        e[:,j] = 1.\n",
    "        z, mu_q, logvar_q = encoder(x,e)\n",
    "        kld_norm += torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1)\n",
    "        log_prob_e.append(decoder(z))\n",
    "        e[:,j] = 0.\n",
    "        \n",
    "    kld_norm = torch.mean(kld_norm, -1))\n",
    "\n",
    "    log_prob_e = torch.floatTensor(log_prob_e)\n",
    "    log_probs = torch.matmul(llk_e,y_q).squeeze()\n",
    "    \n",
    "    loss = nn.NLLLoss()\n",
    "    llk = loss(log_probs,x)\n",
    "    \n",
    "    return llk + kld_cat + kld_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-829e11dca8e4>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-829e11dca8e4>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    log_probs = decoder(x)\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def supervised_loss(x,y,encoder,decoder,classifier):\n",
    "    y_q = classifier(x)\n",
    "    \n",
    "    z, mu_q, logvar_q = encoder(x,y)\n",
    "    kld_norm = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1),-1)\n",
    "\n",
    "    log_probs = decoder(x)\n",
    "                          \n",
    "    loss = nn.NLLLoss()\n",
    "    llk = loss(log_probs,x)\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    llk_cat = loss(y_q,y)\n",
    "                          \n",
    "    return llk + llk_cat + kld_norm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSVAE(nn.Module):\n",
    "    def __init__(self,input_size,num_classes,latent_size, y_prior = 1):\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        self.y_prior = y_prior\n",
    "        \n",
    "        self.encoder = Encoder(input_size,num_classes,latent_size)\n",
    "        self.decoder = Decoder(latent_size,input_size)\n",
    "        self.classifier = Classifier(input_size, num_classes)\n",
    "        \n",
    "        llk_loss = nn.NLLLoss()\n",
    "        cat_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def unsupervised_loss(self, x):\n",
    "        y_q = self.classifier(x)\n",
    "        kld_cat = torch.mean(torch.sum(y_q*(torch.log(y_q) - torch.log(self.y_prior)),-1),-1)  \n",
    "        kld_norm = 0\n",
    "    \n",
    "        e = torch.zeros(y_q.size()).to(x.device)\n",
    "    \n",
    "        log_prob_e = []\n",
    "        for j in range(self.num_classes):\n",
    "            e[:,j] = 1.\n",
    "            z, mu_q, logvar_q = self.encoder(x,e)\n",
    "            kld_norm += torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1)\n",
    "            log_prob_e.append(self.decoder(z))\n",
    "            e[:,j] = 0.\n",
    "        \n",
    "        kld_norm = torch.mean(kld_norm, -1))\n",
    "\n",
    "        log_prob_e = torch.floatTensor(log_prob_e)\n",
    "        log_probs = torch.matmul(llk_e,y_q).squeeze()\n",
    "    \n",
    "        llk = llk_loss(log_probs,x)\n",
    "    \n",
    "        return llk + kld_cat + kld_norm\n",
    "        \n",
    "        \n",
    "    def supervised_loss(self,x,y):\n",
    "        z, mu_q, logvar_q = self.encoder(x,y)\n",
    "        kld_norm = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1),-1)\n",
    "\n",
    "        log_probs = self.decoder(x)                          \n",
    "        llk = loss(log_probs,x)\n",
    "    \n",
    "        y_q = self.classifier(x)\n",
    "        llk_cat = cat_loss(y_q,y)\n",
    "                          \n",
    "        return llk + llk_cat + kld_norm    \n",
    "        \n",
    "    def forward(self, x, y = None, train = False)\n",
    "        if not train:\n",
    "            return self.classifier(x)\n",
    "        else:\n",
    "            if y is not None:\n",
    "                loss = self.supervised_loss(x,y)\n",
    "            else\n",
    "                loss = self.unsupervised_loss(x)\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader,input_size, num_classes, latent_size, y_priors):\n",
    "    model = SSVAE(input_size,num_classes, latent_size)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "    \n",
    "    for batch_idx,(x,y) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model(x,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (ipc, cuda)",
   "language": "python",
   "name": "ptc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
